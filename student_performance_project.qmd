---
title: "Predicting Student Exam Scores using Machine Learning Models"
author: "Mba Solomon Akachukwu"
date: "15th December 2025"
format: pdf
---

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler
```

```{python}
def calculate_mse(y_true, y_pred):
    """Calculate MSE"""
    return float(np.mean((y_true - y_pred) ** 2))

def calculate_rmse(y_true, y_pred):
    """Calculate RMSE"""
    return float(np.sqrt(calculate_mse(y_true, y_pred)))

def calculate_mae(y_true, y_pred):
    """Calculate Mean Absolute Error"""
    return float(np.mean(np.abs(y_true - y_pred)))

def calculate_r2(y_true, y_pred):
    """Calculate R-squared"""
    ss_res = np.sum((y_true - y_pred) ** 2)
    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
    return float(1 - ss_res / ss_tot) if ss_tot > 0 else 0.0
```

```{python}
df = pd.read_csv("StudentPerformanceFactors.csv") 
print("\nFirst few rows:")
print(df.head())
```

```{python}
print(f"Shape: {df.shape}")
print(f"\nColumn types:")
print(df.dtypes)
print(f"\nMissing values (count):")
print(df.isnull().sum())
```

```{python}
quantitative = [
    "Hours_Studied",
    "Attendance",
    "Sleep_Hours",
    "Previous_Scores",
    "Tutoring_Sessions",
    "Physical_Activity",
    "Exam_Score",
]
available_quant = []
for c in quantitative:
    if c in df.columns:
        available_quant.append(c)
print("\nSummary quantitative statistics:")
print(df[available_quant].describe())
```

## Data Preprocessing

```{python}
# Handle missing values
df_proc = df.copy()
df_proc.replace("", np.nan, inplace=True)

target_col = "Exam_Score"
numeric_cols = [
    "Hours_Studied",
    "Attendance",
    "Sleep_Hours",
    "Previous_Scores",
    "Tutoring_Sessions",
    "Physical_Activity",
]
categorical_cols = [
    "Parental_Involvement",
    "Access_to_Resources",
    "Extracurricular_Activities",
    "Motivation_Level",
    "Internet_Access",
    "Family_Income",
    "Teacher_Quality",
    "School_Type",
    "Peer_Influence",
    "Learning_Disabilities",
    "Parental_Education_Level",
    "Distance_from_Home",
    "Gender",
]

for col in df_proc.columns:
    if df_proc[col].isnull().any():
        if col in numeric_cols:
            median_val = df_proc[col].median()
            df_proc[col].fillna(median_val, inplace=True)
        elif col != target_col:
            mode_val = df_proc[col].mode()[0]
            df_proc[col].fillna(mode_val, inplace=True)

y = df_proc[target_col].values.astype(float)
X_df = df_proc.drop(columns=[target_col])

# One-hot encode categoricals
X_encoded = pd.get_dummies(X_df, columns=categorical_cols, drop_first=True)
feature_names = X_encoded.columns.tolist()
X = X_encoded.values.astype(float)

print(f"\nFeatures after encoding: {X.shape[1]}")
```

## Exploratory Data Visualizations

```{python}
# Histogram of Exam_Score
plt.figure(figsize=(8, 5))
plt.hist(df["Exam_Score"], bins=30, alpha=0.7)
plt.xlabel("Exam Score")
plt.ylabel("Frequency")
plt.title("Distribution of Exam Scores")
plt.tight_layout()
plt.show()
```

```{python}
# Scatterplots vs Exam_Score
if "Hours_Studied" in df.columns:
    plt.figure(figsize=(7, 5))
    plt.scatter(df["Hours_Studied"], df["Exam_Score"], alpha=0.5)
    corr = np.corrcoef(df["Hours_Studied"], df["Exam_Score"])[0, 1]
    plt.xlabel("Hours_Studied")
    plt.ylabel("Exam Score")
    plt.title(f"Hours_Studied vs Exam Score (r={corr:.3f})")
    plt.tight_layout()
    plt.show()
```

```{python}
# Correlation heatmap for quantitative variables
quant_cols = [
    "Hours_Studied",
    "Attendance",
    "Sleep_Hours",
    "Previous_Scores",
    "Tutoring_Sessions",
    "Physical_Activity",
    "Exam_Score",
]
quant_cols = [c for c in quant_cols if c in df.columns]
corr = df[quant_cols].corr()
plt.figure(figsize=(8, 6))
sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm", square=True)
plt.title("Correlation Heatmap for Quantitative Features")
plt.tight_layout()
plt.show()
```

```{python}
def cross_validate_model(model_class, X, y, lambdas, cv_folds=5, max_iter=10000):
    kf = KFold(n_splits=cv_folds, shuffle=True, random_state=123)
    scores = {}
    for lam in lambdas:
        fold_mse = []
        for train_idx, val_idx in kf.split(X):
            Xtr, Xval = X[train_idx], X[val_idx]
            ytr, yval = y[train_idx], y[val_idx]
            model = model_class(alpha=lam, max_iter=max_iter)
            model.fit(Xtr, ytr)
            preds = model.predict(Xval)
            fold_mse.append(calculate_mse(yval, preds))
        scores[lam] = float(np.mean(fold_mse))
    best_lam = min(scores, key=scores.get)
    return best_lam, scores
```

```{python}
# Set random seed for reproducibility
np.random.seed(42)
# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=123
)
print(f"Training set: {X_train.shape[0]} samples")
print(f"Test set: {X_test.shape[0]} samples")
# Standardize features
X_train_mean = np.mean(X_train, axis=0)
X_train_std = np.std(X_train, axis=0)
X_train_s = (X_train - X_train_mean) / X_train_std
X_test_s = (X_test - X_train_mean) / X_train_std
```

```{python}
def evaluate_and_report(name, y_true, y_pred):
    mse = calculate_mse(y_true, y_pred)
    rmse = calculate_rmse(y_true, y_pred)
    mae = calculate_mae(y_true, y_pred)
    r2 = calculate_r2(y_true, y_pred)
    print(f"\n{name} performance:")
    print(f"  MSE : {mse:.4f}")
    print(f"  RMSE: {rmse:.4f}")
    print(f"  MAE : {mae:.4f}")
    print(f"  R^2 : {r2:.4f}")
    return {"MSE": mse, "RMSE": rmse, "MAE": mae, "R2": r2}

results = {}
```

```{python}
ols = LinearRegression()
ols.fit(X_train_s, y_train)
ols_test_pred = ols.predict(X_test_s)
results["OLS"] = evaluate_and_report("OLS Test", y_test, ols_test_pred)
residuals_ols = y_test - ols_test_pred
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.scatter(ols_test_pred, residuals_ols, alpha=0.5)
plt.axhline(0, color="red", linestyle="--")
plt.xlabel("Predicted")
plt.ylabel("Residual")
plt.title("OLS - Residuals vs Predicted")
plt.subplot(1, 2, 2)
plt.hist(residuals_ols, bins=30, edgecolor="black", alpha=0.7)
plt.xlabel("Residual")
plt.title("OLS - Residual Distribution")
plt.tight_layout()
plt.show()
```

```{python}
# Cross-validate Ridge
r_lambda_grid = 10**np.linspace(10, -2, 100)
best_ridge_alpha, ridge_scores = cross_validate_model(Ridge, X_train_s, y_train, r_lambda_grid, cv_folds=5, max_iter=10000)
print(f"\nBest Ridge alpha: {best_ridge_alpha:.4f}")
print(f"Best CV MSE: {ridge_scores[best_ridge_alpha]:.4f}")
```

```{python}
# Plot CV scores for Ridge
alphas_ridge = list(ridge_scores.keys())
mses_ridge = [ridge_scores[a] for a in alphas_ridge]
plt.figure(figsize=(8, 5))
plt.semilogx(alphas_ridge, mses_ridge, marker="o")
plt.axvline(best_ridge_alpha, color="red", linestyle="--", 
            label=f"Best alpha={best_ridge_alpha:.4f}")
plt.xlabel("Alpha")
plt.ylabel("Mean CV MSE")
plt.title("Ridge Regression: Cross-Validation MSE vs Alpha")
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

```{python}
# Train Ridge with best alpha
ridge = Ridge(alpha=best_ridge_alpha, max_iter=10000)
ridge.fit(X_train_s, y_train)
ridge_test_pred = ridge.predict(X_test_s)
results["Ridge"] = evaluate_and_report("Ridge Test", y_test, ridge_test_pred)
```

```{python}
# Residual plots for Ridge
residuals_ridge = y_test - ridge_test_pred
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.scatter(ridge_test_pred, residuals_ridge, alpha=0.5)
plt.axhline(0, color="red", linestyle="--")
plt.xlabel("Predicted")
plt.ylabel("Residual")
plt.title("Ridge - Residuals vs Predicted")
plt.subplot(1, 2, 2)
plt.hist(residuals_ridge, bins=30, edgecolor="black", alpha=0.7)
plt.xlabel("Residual")
plt.title("Ridge - Residual Distribution")
plt.tight_layout()
plt.show()
```

```{python}
# Cross-validate Lasso
lasso_lambda_grid = 10**np.linspace(10, -2, 100)
best_lasso_alpha, lasso_scores = cross_validate_model(Lasso, X_train_s, y_train, lasso_lambda_grid, cv_folds=5, max_iter=10000)
print(f"\nBest Lasso alpha: {best_lasso_alpha:.4f}")
print(f"Best CV MSE: {lasso_scores[best_lasso_alpha]:.4f}")
```

```{python}
# Plot CV scores for Lasso
alphas_lasso = list(lasso_scores.keys())
mses_lasso = [lasso_scores[a] for a in alphas_lasso]
plt.figure(figsize=(8, 5))
plt.semilogx(alphas_lasso, mses_lasso, marker="o", color="green")
plt.axvline(best_lasso_alpha, color="red", linestyle="--", 
            label=f"Best alpha={best_lasso_alpha:.4f}")
plt.xlabel("Alpha (log scale)")
plt.ylabel("Mean CV MSE")
plt.title("Lasso Regression: Cross-Validation MSE vs Alpha")
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

